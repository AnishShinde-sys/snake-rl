{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêç Snake RL with MCTS + PPO\n",
        "\n",
        "AlphaZero-style training for Snake game using:\n",
        "- **PPO** (Proximal Policy Optimization) for policy learning\n",
        "- **MCTS** (Monte Carlo Tree Search) for action selection\n",
        "\n",
        "## Setup\n",
        "Run all cells to train your agent. **GPU is recommended!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch -q\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Snake Environment\n",
        "30-feature state representation with relative actions (straight/right/left)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class SnakeEnv:\n",
        "    \"\"\"Snake environment with 30-feature state\"\"\"\n",
        "    DIRECTIONS = [(0, -1), (1, 0), (0, 1), (-1, 0)]\n",
        "    \n",
        "    def __init__(self, grid_size=10):\n",
        "        self.grid_size = grid_size\n",
        "        self.action_space = 3\n",
        "        self.observation_space = 30\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.snake = deque([(self.grid_size // 2, self.grid_size // 2)])\n",
        "        self.direction = 1\n",
        "        self.food = self._place_food()\n",
        "        self.score = 0\n",
        "        self.steps = 0\n",
        "        self.max_steps = self.grid_size * self.grid_size * 20\n",
        "        return self._get_state()\n",
        "    \n",
        "    def _place_food(self):\n",
        "        while True:\n",
        "            food = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n",
        "            if food not in self.snake:\n",
        "                return food\n",
        "    \n",
        "    def _is_collision(self, pos):\n",
        "        if pos[0] < 0 or pos[0] >= self.grid_size or pos[1] < 0 or pos[1] >= self.grid_size:\n",
        "            return True\n",
        "        return pos in list(self.snake)[:-1]\n",
        "    \n",
        "    def _get_depth(self, start_pos, direction_idx):\n",
        "        dx, dy = self.DIRECTIONS[direction_idx]\n",
        "        x, y = start_pos\n",
        "        distance = 0\n",
        "        while True:\n",
        "            x += dx; y += dy; distance += 1\n",
        "            if x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size: break\n",
        "            if (x, y) in self.snake: break\n",
        "        return distance / self.grid_size\n",
        "    \n",
        "    def _get_wall_distance(self, start_pos, direction_idx):\n",
        "        dx, dy = self.DIRECTIONS[direction_idx]\n",
        "        x, y = start_pos\n",
        "        distance = 0\n",
        "        while True:\n",
        "            x += dx; y += dy; distance += 1\n",
        "            if x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size: break\n",
        "        return distance / self.grid_size\n",
        "    \n",
        "    def _get_next_pos(self, action):\n",
        "        head = self.snake[0]\n",
        "        dx, dy = self.DIRECTIONS[action]\n",
        "        return (head[0] + dx, head[1] + dy)\n",
        "    \n",
        "    def _manhattan_distance(self, pos1, pos2):\n",
        "        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
        "    \n",
        "    def _is_pos_safe(self, pos, snake_body):\n",
        "        return 0 <= pos[0] < self.grid_size and 0 <= pos[1] < self.grid_size and pos not in snake_body\n",
        "    \n",
        "    def _count_safe_moves(self, head_pos, direction, snake_body):\n",
        "        safe_count = 0\n",
        "        for turn in [0, 1, 2]:\n",
        "            new_dir = direction if turn == 0 else (direction + 1) % 4 if turn == 1 else (direction - 1) % 4\n",
        "            dx, dy = self.DIRECTIONS[new_dir]\n",
        "            if self._is_pos_safe((head_pos[0] + dx, head_pos[1] + dy), snake_body):\n",
        "                safe_count += 1\n",
        "        return safe_count\n",
        "    \n",
        "    def _lookahead_safe_moves(self, action_dir, depth=2):\n",
        "        head = self.snake[0]\n",
        "        dx, dy = self.DIRECTIONS[action_dir]\n",
        "        next_head = (head[0] + dx, head[1] + dy)\n",
        "        snake_set = set(self.snake)\n",
        "        if not self._is_pos_safe(next_head, snake_set): return (0, 0)\n",
        "        new_snake = list(self.snake)\n",
        "        new_snake.insert(0, next_head)\n",
        "        if next_head != self.food: new_snake.pop()\n",
        "        new_snake_set = set(new_snake)\n",
        "        safe_after_first = self._count_safe_moves(next_head, action_dir, new_snake_set)\n",
        "        if depth <= 1: return (safe_after_first, safe_after_first)\n",
        "        min_safe, max_safe, paths_found = 3, 0, 0\n",
        "        for turn in [0, 1, 2]:\n",
        "            dir2 = action_dir if turn == 0 else (action_dir + 1) % 4 if turn == 1 else (action_dir - 1) % 4\n",
        "            dx2, dy2 = self.DIRECTIONS[dir2]\n",
        "            pos2 = (next_head[0] + dx2, next_head[1] + dy2)\n",
        "            if self._is_pos_safe(pos2, new_snake_set):\n",
        "                snake2 = list(new_snake)\n",
        "                snake2.insert(0, pos2)\n",
        "                if pos2 != self.food: snake2.pop()\n",
        "                safe_after_second = self._count_safe_moves(pos2, dir2, set(snake2))\n",
        "                min_safe = min(min_safe, safe_after_second)\n",
        "                max_safe = max(max_safe, safe_after_second)\n",
        "                paths_found += 1\n",
        "        return (min_safe, max_safe) if paths_found > 0 else (0, 0)\n",
        "    \n",
        "    def _can_reach_tail(self, head_pos, snake_body_set, tail_pos):\n",
        "        if head_pos == tail_pos: return True\n",
        "        visited = set([head_pos])\n",
        "        queue = [head_pos]\n",
        "        while queue:\n",
        "            pos = queue.pop(0)\n",
        "            for dx, dy in self.DIRECTIONS:\n",
        "                new_pos = (pos[0] + dx, pos[1] + dy)\n",
        "                if new_pos == tail_pos: return True\n",
        "                if new_pos not in visited and 0 <= new_pos[0] < self.grid_size and 0 <= new_pos[1] < self.grid_size and new_pos not in snake_body_set:\n",
        "                    visited.add(new_pos)\n",
        "                    queue.append(new_pos)\n",
        "        return False\n",
        "    \n",
        "    def get_action_mask(self):\n",
        "        head = self.snake[0]\n",
        "        snake_set = set(self.snake)\n",
        "        snake_len = len(self.snake)\n",
        "        mask = [False, False, False]\n",
        "        best_options = [-1, -1, -1]\n",
        "        can_reach_tail_score = [0, 0, 0]\n",
        "        food_adjacent = [False, False, False]\n",
        "        for action in range(3):\n",
        "            abs_dir = self.direction if action == 0 else (self.direction + 1) % 4 if action == 1 else (self.direction - 1) % 4\n",
        "            dx, dy = self.DIRECTIONS[abs_dir]\n",
        "            next_pos = (head[0] + dx, head[1] + dy)\n",
        "            if not self._is_pos_safe(next_pos, snake_set):\n",
        "                best_options[action] = -100\n",
        "                continue\n",
        "            new_snake_list = [next_pos] + list(self.snake)\n",
        "            eating_food = (next_pos == self.food)\n",
        "            food_adjacent[action] = eating_food\n",
        "            if not eating_food: new_snake_list.pop()\n",
        "            new_snake_set = set(new_snake_list)\n",
        "            new_tail = new_snake_list[-1]\n",
        "            safe_after = self._count_safe_moves(next_pos, abs_dir, new_snake_set)\n",
        "            best_options[action] = safe_after\n",
        "            if eating_food:\n",
        "                best_options[action] += 200\n",
        "                can_reach_tail_score[action] = 100\n",
        "            elif snake_len > 30:\n",
        "                if self._can_reach_tail(next_pos, new_snake_set - {new_tail}, new_tail):\n",
        "                    can_reach_tail_score[action] = 100\n",
        "                    best_options[action] += 100\n",
        "                else:\n",
        "                    best_options[action] -= 50\n",
        "            if safe_after >= 1: mask[action] = True\n",
        "        for action in range(3):\n",
        "            if food_adjacent[action] and best_options[action] > 0: mask[action] = True\n",
        "        if snake_len > 30 and not any(food_adjacent):\n",
        "            tail_reachable_mask = [can_reach_tail_score[i] > 0 for i in range(3)]\n",
        "            if any(tail_reachable_mask): mask = tail_reachable_mask\n",
        "        if not any(mask):\n",
        "            mask[max(range(3), key=lambda i: best_options[i])] = True\n",
        "        return mask\n",
        "    \n",
        "    def _get_state(self):\n",
        "        head = self.snake[0]\n",
        "        features = []\n",
        "        relative_dirs = [self.direction, (self.direction + 1) % 4, (self.direction - 1) % 4]\n",
        "        # Danger (3)\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(1 if self._is_collision(self._get_next_pos(abs_dir)) else 0)\n",
        "        # Food direction (3)\n",
        "        food_dx, food_dy = self.food[0] - head[0], self.food[1] - head[1]\n",
        "        for abs_dir in relative_dirs:\n",
        "            dir_dx, dir_dy = self.DIRECTIONS[abs_dir]\n",
        "            features.append(1 if food_dx * dir_dx + food_dy * dir_dy > 0 else 0)\n",
        "        # Food distance (1)\n",
        "        features.append(self._manhattan_distance(head, self.food) / (2 * self.grid_size))\n",
        "        # Wall distance (3)\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(self._get_wall_distance(head, abs_dir))\n",
        "        # Depth (3)\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(self._get_depth(head, abs_dir))\n",
        "        # Would die (3)\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(1 if self._is_collision(self._get_next_pos(abs_dir)) else 0)\n",
        "        # Would eat (3)\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(1 if self._get_next_pos(abs_dir) == self.food else 0)\n",
        "        # Distance change (3)\n",
        "        current_dist = self._manhattan_distance(head, self.food)\n",
        "        for abs_dir in relative_dirs:\n",
        "            next_pos = self._get_next_pos(abs_dir)\n",
        "            if self._is_collision(next_pos): features.append(1)\n",
        "            else: features.append((self._manhattan_distance(next_pos, self.food) - current_dist) / (2 * self.grid_size))\n",
        "        # Snake length (1)\n",
        "        features.append(len(self.snake) / (self.grid_size * self.grid_size))\n",
        "        # Adjacent body (1)\n",
        "        adjacent_body = sum(1 for d in range(4) if (head[0] + self.DIRECTIONS[d][0], head[1] + self.DIRECTIONS[d][1]) in list(self.snake)[1:])\n",
        "        features.append(adjacent_body / 4)\n",
        "        # Lookahead (6)\n",
        "        for abs_dir in relative_dirs:\n",
        "            min_safe, max_safe = self._lookahead_safe_moves(abs_dir, depth=2)\n",
        "            features.append(min_safe / 3.0)\n",
        "            features.append(max_safe / 3.0)\n",
        "        return np.array(features, dtype=np.float32)\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        new_direction = self.direction if action == 0 else (self.direction + 1) % 4 if action == 1 else (self.direction - 1) % 4\n",
        "        self.direction = new_direction\n",
        "        head = self.snake[0]\n",
        "        dx, dy = self.DIRECTIONS[new_direction]\n",
        "        new_head = (head[0] + dx, head[1] + dy)\n",
        "        if new_head[0] < 0 or new_head[0] >= self.grid_size or new_head[1] < 0 or new_head[1] >= self.grid_size:\n",
        "            return self._get_state(), -10, True, {\"score\": self.score, \"reason\": \"wall\"}\n",
        "        if new_head in list(self.snake)[:-1]:\n",
        "            return self._get_state(), -10, True, {\"score\": self.score, \"reason\": \"self\"}\n",
        "        self.snake.appendleft(new_head)\n",
        "        if new_head == self.food:\n",
        "            self.score += 1\n",
        "            reward = 10\n",
        "            self.food = self._place_food()\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "            reward = 1 if self._manhattan_distance(new_head, self.food) < self._manhattan_distance(head, self.food) else -1.5\n",
        "            snake_len = len(self.snake)\n",
        "            if snake_len > 10: reward += 0.1 * (snake_len / 10)\n",
        "            snake_set = set(self.snake)\n",
        "            safe_moves = self._count_safe_moves(new_head, new_direction, snake_set)\n",
        "            if safe_moves == 0: reward -= 5\n",
        "            elif safe_moves == 1 and snake_len > 5: reward -= 2\n",
        "            elif safe_moves == 2 and snake_len > 10: reward -= 0.5\n",
        "            if snake_len > 30:\n",
        "                tail = self.snake[-1]\n",
        "                if self._can_reach_tail(new_head, snake_set - {tail}, tail): reward += 2.0\n",
        "                else: reward -= 5.0\n",
        "        if self.steps >= self.max_steps:\n",
        "            return self._get_state(), -5, True, {\"score\": self.score, \"reason\": \"timeout\"}\n",
        "        return self._get_state(), reward, False, {\"score\": self.score}\n",
        "\n",
        "print(\"‚úì SnakeEnv loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPO Agent\n",
        "Actor-Critic network with separate networks for policy and value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim), nn.Softmax(dim=-1))\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1))\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for module in self.actor:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "        nn.init.orthogonal_(self.actor[-2].weight, gain=0.01)\n",
        "        for module in self.critic:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        return self.actor(state), self.critic(state)\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, eps_clip=0.2, k_epochs=10, device='cpu'):\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.k_epochs = k_epochs\n",
        "        self.device = device\n",
        "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
        "        self.lr = lr\n",
        "        self.min_lr = lr * 0.1\n",
        "        self.actor_optimizer = optim.Adam(self.policy.actor.parameters(), lr=lr)\n",
        "        self.critic_optimizer = optim.Adam(self.policy.critic.parameters(), lr=lr)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.update_count = 0\n",
        "        self.states, self.actions, self.logprobs, self.rewards, self.is_terminals, self.state_values = [], [], [], [], [], []\n",
        "    \n",
        "    def select_action(self, state, action_mask=None):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).to(self.device)\n",
        "            action_probs, state_value = self.policy_old(state_tensor)\n",
        "            if action_mask is not None:\n",
        "                mask_tensor = torch.FloatTensor(action_mask).to(self.device)\n",
        "                masked_probs = action_probs * mask_tensor\n",
        "                if masked_probs.sum() > 0:\n",
        "                    masked_probs = masked_probs / masked_probs.sum()\n",
        "                else:\n",
        "                    masked_probs = action_probs\n",
        "                action_probs_to_use = masked_probs\n",
        "            else:\n",
        "                action_probs_to_use = action_probs\n",
        "            dist = Categorical(action_probs_to_use)\n",
        "            action = dist.sample()\n",
        "            action_logprob = dist.log_prob(action)\n",
        "            action = action.item()\n",
        "        self.states.append(state_tensor)\n",
        "        self.actions.append(action)\n",
        "        self.logprobs.append(action_logprob)\n",
        "        self.state_values.append(state_value)\n",
        "        return action\n",
        "    \n",
        "    def store_transition(self, reward, is_terminal):\n",
        "        self.rewards.append(reward)\n",
        "        self.is_terminals.append(is_terminal)\n",
        "    \n",
        "    def update(self):\n",
        "        if len(self.rewards) == 0: return\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.rewards), reversed(self.is_terminals)):\n",
        "            if is_terminal: discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
        "        old_states = torch.stack(self.states).detach()\n",
        "        old_actions = torch.tensor(self.actions, device=self.device)\n",
        "        old_logprobs = torch.stack(self.logprobs).detach()\n",
        "        old_state_values = torch.stack(self.state_values).squeeze().detach()\n",
        "        advantages = rewards - old_state_values\n",
        "        if len(advantages) > 1:\n",
        "            advantages = advantages / (advantages.std() + 1e-7)\n",
        "        for _ in range(self.k_epochs):\n",
        "            action_probs, state_values = self.policy(old_states)\n",
        "            dist = Categorical(action_probs)\n",
        "            action_logprobs = dist.log_prob(old_actions)\n",
        "            dist_entropy = dist.entropy()\n",
        "            ratios = torch.exp(action_logprobs - old_logprobs)\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "            value_loss = self.mse_loss(state_values.squeeze(), rewards)\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "            actor_loss = policy_loss - 0.05 * dist_entropy.mean()\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward(retain_graph=True)\n",
        "            self.actor_optimizer.step()\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            value_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.policy.critic.parameters(), 0.5)\n",
        "            self.critic_optimizer.step()\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        self.update_count += 1\n",
        "        if self.update_count % 50 == 0:\n",
        "            new_lr = max(self.min_lr, self.lr * (0.995 ** (self.update_count // 50)))\n",
        "            for pg in self.actor_optimizer.param_groups: pg['lr'] = new_lr\n",
        "            for pg in self.critic_optimizer.param_groups: pg['lr'] = new_lr\n",
        "        self.states, self.actions, self.logprobs, self.rewards, self.is_terminals, self.state_values = [], [], [], [], [], []\n",
        "    \n",
        "    def save(self, filepath):\n",
        "        torch.save({'policy_state_dict': self.policy.state_dict(),\n",
        "                   'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
        "                   'critic_optimizer_state_dict': self.critic_optimizer.state_dict()}, filepath)\n",
        "    \n",
        "    def load(self, filepath):\n",
        "        checkpoint = torch.load(filepath, map_location=self.device)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.policy_old.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        if 'actor_optimizer_state_dict' in checkpoint:\n",
        "            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
        "            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
        "\n",
        "print(\"‚úì PPOAgent loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCTS\n",
        "AlphaZero-style tree search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class MCTSNode:\n",
        "    def __init__(self, state, parent=None, action=None, prior=0.0):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.prior = prior\n",
        "        self.children = {}\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0.0\n",
        "        self.is_terminal = False\n",
        "        self.terminal_value = 0.0\n",
        "    \n",
        "    @property\n",
        "    def value(self):\n",
        "        return 0.0 if self.visit_count == 0 else self.value_sum / self.visit_count\n",
        "    \n",
        "    def is_expanded(self):\n",
        "        return len(self.children) > 0\n",
        "    \n",
        "    def select_child(self, c_puct=1.5):\n",
        "        best_score = -float('inf')\n",
        "        best_child = None\n",
        "        sqrt_total = math.sqrt(self.visit_count + 1)\n",
        "        for action, child in self.children.items():\n",
        "            q_value = child.value if child.visit_count > 0 else 0.0\n",
        "            exploration = c_puct * child.prior * sqrt_total / (1 + child.visit_count)\n",
        "            score = q_value + exploration\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_child = child\n",
        "        return best_child.action if best_child else None, best_child\n",
        "    \n",
        "    def expand(self, action_priors, next_states, terminals, terminal_values):\n",
        "        for action, prior in action_priors.items():\n",
        "            if action not in self.children:\n",
        "                child = MCTSNode(state=next_states[action], parent=self, action=action, prior=prior)\n",
        "                child.is_terminal = terminals[action]\n",
        "                child.terminal_value = terminal_values[action]\n",
        "                self.children[action] = child\n",
        "\n",
        "class SnakeSimulator:\n",
        "    DIRECTIONS = [(0, -1), (1, 0), (0, 1), (-1, 0)]\n",
        "    def __init__(self, grid_size=10):\n",
        "        self.grid_size = grid_size\n",
        "    \n",
        "    def get_state_dict(self, env):\n",
        "        return {'snake': list(env.snake), 'food': env.food, 'direction': env.direction,\n",
        "                'score': env.score, 'steps': env.steps, 'max_steps': env.max_steps}\n",
        "    \n",
        "    def simulate_action(self, state_dict, action):\n",
        "        snake = deque(state_dict['snake'])\n",
        "        food = state_dict['food']\n",
        "        direction = state_dict['direction']\n",
        "        score = state_dict['score']\n",
        "        steps = state_dict['steps'] + 1\n",
        "        max_steps = state_dict['max_steps']\n",
        "        new_direction = direction if action == 0 else (direction + 1) % 4 if action == 1 else (direction - 1) % 4\n",
        "        head = snake[0]\n",
        "        dx, dy = self.DIRECTIONS[new_direction]\n",
        "        new_head = (head[0] + dx, head[1] + dy)\n",
        "        if new_head[0] < 0 or new_head[0] >= self.grid_size or new_head[1] < 0 or new_head[1] >= self.grid_size:\n",
        "            return state_dict, -10, True, {'score': score, 'reason': 'wall'}\n",
        "        if new_head in list(snake)[:-1]:\n",
        "            return state_dict, -10, True, {'score': score, 'reason': 'self'}\n",
        "        new_snake = deque([new_head] + list(snake))\n",
        "        if new_head == food:\n",
        "            score += 1\n",
        "            reward = 10\n",
        "            new_food = self._place_food(new_snake)\n",
        "        else:\n",
        "            new_snake.pop()\n",
        "            new_food = food\n",
        "            old_dist = abs(head[0] - food[0]) + abs(head[1] - food[1])\n",
        "            new_dist = abs(new_head[0] - food[0]) + abs(new_head[1] - food[1])\n",
        "            reward = 1 if new_dist < old_dist else -1.5\n",
        "        if steps >= max_steps:\n",
        "            return {'snake': list(new_snake), 'food': new_food, 'direction': new_direction,\n",
        "                    'score': score, 'steps': steps, 'max_steps': max_steps}, -5, True, {'score': score, 'reason': 'timeout'}\n",
        "        return {'snake': list(new_snake), 'food': new_food, 'direction': new_direction,\n",
        "                'score': score, 'steps': steps, 'max_steps': max_steps}, reward, False, {'score': score}\n",
        "    \n",
        "    def _place_food(self, snake):\n",
        "        snake_set = set(snake)\n",
        "        empty = [(x, y) for x in range(self.grid_size) for y in range(self.grid_size) if (x, y) not in snake_set]\n",
        "        return empty[np.random.randint(len(empty))] if empty else None\n",
        "    \n",
        "    def get_features(self, state_dict):\n",
        "        snake = deque(state_dict['snake'])\n",
        "        food = state_dict['food']\n",
        "        direction = state_dict['direction']\n",
        "        head = snake[0]\n",
        "        features = []\n",
        "        relative_dirs = [direction, (direction + 1) % 4, (direction - 1) % 4]\n",
        "        for abs_dir in relative_dirs:\n",
        "            dx, dy = self.DIRECTIONS[abs_dir]\n",
        "            next_pos = (head[0] + dx, head[1] + dy)\n",
        "            features.append(1 if self._is_collision(next_pos, snake) else 0)\n",
        "        food_dx, food_dy = food[0] - head[0], food[1] - head[1]\n",
        "        for abs_dir in relative_dirs:\n",
        "            dir_dx, dir_dy = self.DIRECTIONS[abs_dir]\n",
        "            features.append(1 if food_dx * dir_dx + food_dy * dir_dy > 0 else 0)\n",
        "        features.append((abs(head[0] - food[0]) + abs(head[1] - food[1])) / (2 * self.grid_size))\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(self._get_wall_distance(head, abs_dir))\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(self._get_depth(head, abs_dir, snake))\n",
        "        for abs_dir in relative_dirs:\n",
        "            dx, dy = self.DIRECTIONS[abs_dir]\n",
        "            features.append(1 if self._is_collision((head[0] + dx, head[1] + dy), snake) else 0)\n",
        "        for abs_dir in relative_dirs:\n",
        "            dx, dy = self.DIRECTIONS[abs_dir]\n",
        "            features.append(1 if (head[0] + dx, head[1] + dy) == food else 0)\n",
        "        current_dist = abs(head[0] - food[0]) + abs(head[1] - food[1])\n",
        "        for abs_dir in relative_dirs:\n",
        "            dx, dy = self.DIRECTIONS[abs_dir]\n",
        "            next_pos = (head[0] + dx, head[1] + dy)\n",
        "            if self._is_collision(next_pos, snake): features.append(1)\n",
        "            else:\n",
        "                new_dist = abs(next_pos[0] - food[0]) + abs(next_pos[1] - food[1])\n",
        "                features.append((new_dist - current_dist) / (2 * self.grid_size))\n",
        "        features.append(len(snake) / (self.grid_size * self.grid_size))\n",
        "        adjacent = sum(1 for d in range(4) if (head[0] + self.DIRECTIONS[d][0], head[1] + self.DIRECTIONS[d][1]) in list(snake)[1:])\n",
        "        features.append(adjacent / 4)\n",
        "        for abs_dir in relative_dirs:\n",
        "            features.append(0.5)\n",
        "            features.append(0.5)\n",
        "        return np.array(features, dtype=np.float32)\n",
        "    \n",
        "    def _is_collision(self, pos, snake):\n",
        "        if pos[0] < 0 or pos[0] >= self.grid_size or pos[1] < 0 or pos[1] >= self.grid_size: return True\n",
        "        return pos in list(snake)[:-1]\n",
        "    \n",
        "    def _get_wall_distance(self, pos, direction):\n",
        "        dx, dy = self.DIRECTIONS[direction]\n",
        "        x, y = pos\n",
        "        distance = 0\n",
        "        while True:\n",
        "            x += dx; y += dy; distance += 1\n",
        "            if x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size: break\n",
        "        return distance / self.grid_size\n",
        "    \n",
        "    def _get_depth(self, pos, direction, snake):\n",
        "        dx, dy = self.DIRECTIONS[direction]\n",
        "        x, y = pos\n",
        "        distance = 0\n",
        "        while True:\n",
        "            x += dx; y += dy; distance += 1\n",
        "            if x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size: break\n",
        "            if (x, y) in snake: break\n",
        "        return distance / self.grid_size\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, policy_network, grid_size=10, c_puct=1.5, num_simulations=100, device='cpu'):\n",
        "        self.policy_network = policy_network\n",
        "        self.grid_size = grid_size\n",
        "        self.c_puct = c_puct\n",
        "        self.num_simulations = num_simulations\n",
        "        self.device = device\n",
        "        self.simulator = SnakeSimulator(grid_size)\n",
        "    \n",
        "    def get_action_probs(self, env, temperature=1.0):\n",
        "        state_dict = self.simulator.get_state_dict(env)\n",
        "        root = MCTSNode(state=state_dict)\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "            while node.is_expanded() and not node.is_terminal:\n",
        "                _, node = node.select_child(self.c_puct)\n",
        "                if node: search_path.append(node)\n",
        "                else: break\n",
        "            if node and node.is_terminal:\n",
        "                value = node.terminal_value\n",
        "            elif node:\n",
        "                value = self._expand_node(node)\n",
        "            else:\n",
        "                value = 0\n",
        "            for n in reversed(search_path):\n",
        "                n.visit_count += 1\n",
        "                n.value_sum += value\n",
        "        visits = np.array([root.children[a].visit_count if a in root.children else 0 for a in range(3)])\n",
        "        if temperature == 0 or visits.sum() == 0:\n",
        "            action_probs = np.zeros(3)\n",
        "            action_probs[np.argmax(visits)] = 1.0\n",
        "        else:\n",
        "            visits_temp = visits ** (1.0 / temperature)\n",
        "            action_probs = visits_temp / (visits_temp.sum() + 1e-8)\n",
        "        return action_probs, np.argmax(visits)\n",
        "    \n",
        "    def _expand_node(self, node):\n",
        "        state_dict = node.state\n",
        "        features = self.simulator.get_features(state_dict)\n",
        "        with torch.no_grad():\n",
        "            features_tensor = torch.FloatTensor(features).to(self.device)\n",
        "            action_probs, value = self.policy_network(features_tensor)\n",
        "            action_probs = action_probs.cpu().numpy()\n",
        "            value = value.item()\n",
        "        next_states, terminals, terminal_values, action_priors = {}, {}, {}, {}\n",
        "        for action in range(3):\n",
        "            next_state, reward, done, _ = self.simulator.simulate_action(state_dict, action)\n",
        "            next_states[action] = next_state\n",
        "            terminals[action] = done\n",
        "            terminal_values[action] = reward if done else 0\n",
        "            action_priors[action] = action_probs[action]\n",
        "        node.expand(action_priors, next_states, terminals, terminal_values)\n",
        "        return value\n",
        "    \n",
        "    def select_action(self, env, temperature=1.0):\n",
        "        action_probs, best_action = self.get_action_probs(env, temperature)\n",
        "        return best_action if temperature == 0 else np.random.choice(3, p=action_probs)\n",
        "\n",
        "class HybridAgent:\n",
        "    def __init__(self, ppo_agent, mcts_threshold=50, num_simulations=100):\n",
        "        self.ppo_agent = ppo_agent\n",
        "        self.mcts_threshold = mcts_threshold\n",
        "        self.mcts = MCTS(policy_network=ppo_agent.policy, grid_size=10, num_simulations=num_simulations, device=ppo_agent.device)\n",
        "        self.use_mcts_count = 0\n",
        "        self.use_ppo_count = 0\n",
        "    \n",
        "    def select_action(self, state, env, action_mask=None):\n",
        "        snake_length = len(env.snake)\n",
        "        if snake_length >= self.mcts_threshold:\n",
        "            self.use_mcts_count += 1\n",
        "            action = self.mcts.select_action(env, temperature=0.5)\n",
        "            self._record_action_to_ppo(state, action, action_mask)\n",
        "            return action\n",
        "        else:\n",
        "            self.use_ppo_count += 1\n",
        "            return self.ppo_agent.select_action(state, action_mask)\n",
        "    \n",
        "    def _record_action_to_ppo(self, state, mcts_action, action_mask=None):\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).to(self.ppo_agent.device)\n",
        "            action_probs, state_value = self.ppo_agent.policy_old(state_tensor)\n",
        "            if action_mask is not None:\n",
        "                mask_tensor = torch.FloatTensor(action_mask).to(self.ppo_agent.device)\n",
        "                masked_probs = action_probs * mask_tensor\n",
        "                action_probs_to_use = masked_probs / masked_probs.sum() if masked_probs.sum() > 0 else action_probs\n",
        "            else:\n",
        "                action_probs_to_use = action_probs\n",
        "            dist = Categorical(action_probs_to_use)\n",
        "            action_logprob = dist.log_prob(torch.tensor(mcts_action, device=self.ppo_agent.device))\n",
        "        self.ppo_agent.states.append(state_tensor)\n",
        "        self.ppo_agent.actions.append(mcts_action)\n",
        "        self.ppo_agent.logprobs.append(action_logprob)\n",
        "        self.ppo_agent.state_values.append(state_value)\n",
        "    \n",
        "    def store_transition(self, reward, is_terminal):\n",
        "        self.ppo_agent.store_transition(reward, is_terminal)\n",
        "    \n",
        "    def update(self):\n",
        "        self.ppo_agent.update()\n",
        "    \n",
        "    def save(self, filepath):\n",
        "        self.ppo_agent.save(filepath)\n",
        "    \n",
        "    def load(self, filepath):\n",
        "        self.ppo_agent.load(filepath)\n",
        "\n",
        "print(\"‚úì MCTS loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "Adjust parameters below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIGURATION =====\n",
        "MAX_EPISODES = 10000        # Total episodes\n",
        "MCTS_SIMULATIONS = 200      # Simulations per move (more = better but slower)\n",
        "MCTS_THRESHOLD = 1          # 1 = MCTS always, 50+ = MCTS only at high scores\n",
        "UPDATE_FREQUENCY = 10       # PPO update frequency\n",
        "SAVE_INTERVAL = 500         # Checkpoint frequency\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Load checkpoint? Set to None for fresh start\n",
        "LOAD_CHECKPOINT = None  # e.g., \"/content/drive/MyDrive/snake.pt\"\n",
        "\n",
        "print(f\"Episodes: {MAX_EPISODES}, MCTS sims: {MCTS_SIMULATIONS}, threshold: {MCTS_THRESHOLD}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "Run this to start training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Initialize\n",
        "env = SnakeEnv(grid_size=10)\n",
        "ppo_agent = PPOAgent(state_dim=30, action_dim=3, lr=LEARNING_RATE, gamma=0.9,\n",
        "                     eps_clip=0.2, k_epochs=4, device=device)\n",
        "\n",
        "if LOAD_CHECKPOINT:\n",
        "    ppo_agent.load(LOAD_CHECKPOINT)\n",
        "    print(f\"Loaded: {LOAD_CHECKPOINT}\")\n",
        "\n",
        "agent = HybridAgent(ppo_agent=ppo_agent, mcts_threshold=MCTS_THRESHOLD, num_simulations=MCTS_SIMULATIONS)\n",
        "\n",
        "scores = []\n",
        "best_score = 0\n",
        "start_time = datetime.now()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training on {device} | MCTS: {MCTS_SIMULATIONS} sims\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for episode in range(1, MAX_EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action_mask = env.get_action_mask()\n",
        "        action = agent.select_action(state, env, action_mask)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        agent.store_transition(reward, done)\n",
        "    \n",
        "    score = info.get('score', 0)\n",
        "    scores.append(score)\n",
        "    \n",
        "    if episode % UPDATE_FREQUENCY == 0:\n",
        "        agent.update()\n",
        "    \n",
        "    avg = np.mean(scores[-100:]) if scores else 0\n",
        "    elapsed = (datetime.now() - start_time).total_seconds()\n",
        "    reason = info.get('reason', '')\n",
        "    \n",
        "    print(f\"Ep {episode:5d} | Score: {score:3d} | Max: {max(scores):3d} | Avg100: {avg:5.1f} | {elapsed:.0f}s | {reason}\")\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        agent.save('/content/snake_best.pt')\n",
        "        print(f\"  *** NEW BEST: {score} ***\")\n",
        "    \n",
        "    if episode % SAVE_INTERVAL == 0:\n",
        "        agent.save(f'/content/snake_ep{episode}.pt')\n",
        "        print(f\"  Checkpoint saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Done! Avg: {np.mean(scores):.1f}, Max: {max(scores)}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/snake_best.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save to Drive\n",
        "# agent.save('/content/drive/MyDrive/snake_best.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
